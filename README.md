<div align="center">

# MLP From Scratch with GUI

A powerful neural network visualization and training tool built entirely from scratch using pure NumPy. Train, visualize, and experiment with Multi-Layer Perceptrons, Autoencoders, and classic learning algorithms through an intuitive GUI.

[![Python Version](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

</div>

## âœ¨ Key Features

### ğŸ§  **Neural Network Algorithms**
- **Multi-Layer Perceptron (MLP)** - Fully customizable architecture with backpropagation
- **Single Layer Perceptron** - Classic binary/multi-class classifier
- **Delta Rule (Adaline)** - Gradient descent-based learning
- **Autoencoder** - Unsupervised feature extraction and dimensionality reduction
- **Hybrid Autoencoder-MLP** - Two-stage training with encoder reuse

### ğŸ¨ **Interactive Visualization**
- **Real-time decision boundaries** - Watch your network learn
- **Loss/Error graphs** - Track convergence across epochs
- **Reconstruction visualization** - See autoencoder outputs (10 digits in 4Ã—5 grid)
- **Training/Test split** - Separate visualization for validation
- **Modern dark theme** - Built with CustomTkinter

### âœï¸ **MNIST Handwriting Tester**
- **Draw your own digits** - 280Ã—280 canvas for easy drawing
- **Real-time prediction** - Instant classification with confidence
- **MNIST preprocessing** - Automatic centering, resizing, and normalization

### ğŸ’¾ **Model Persistence**
- **Save/Load Models** - Export trained models as `.pkl` files
- **Encoder Save/Load** - Reuse trained autoencoders across sessions
- **Two-stage workflow** - Train encoder â†’ Save â†’ Train MLP separately

### ğŸ“Š **Datasets**
- **Manual 2D Playground** - Click to create custom datasets
- **MNIST** - Handwritten digit recognition (60k train, 10k test)
- **Built-in Presets**:
  - Classification: XOR, Circles, Moons, Blobs
  - Regression: Sine, Parabola, Linear, Absolute

### âš™ï¸ **Advanced Configuration**
- **Flexible architecture** - Define any layer structure (e.g., 784-256-128-10)
- **Activation functions** - ReLU, Sigmoid, Tanh, Softmax
- **Hyperparameter tuning** - Learning rate, batch size, epochs, momentum
- **Stopping criteria** - Converge on min error or max epochs
- **L2 Regularization** - Prevent overfitting
- **Momentum** - Accelerate training with momentum factor

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/ensaryesir/MLP-From-Scratch-GUI.git
cd MLP-From-Scratch-GUI

# Install dependencies
pip install -r requirements.txt

# Run the application
python main.py
```

---

## ğŸ“– Usage Guide

### 1ï¸âƒ£ **Manual Mode** (2D Playground)

Perfect for understanding how neural networks learn decision boundaries.

#### Classification
1. Select **"Classification"** task
2. Select **"Manual"** dataset
3. Choose a model (Perceptron, Delta Rule, or MLP)
4. Add classes and click on canvas to create data points
5. Configure hyperparameters
6. Press **"START TRAINING"**

**Built-in Presets:**
- **XOR Problem** - Classic non-linearly separable dataset
- **Circles** - Concentric circles classification
- **Moons** - Two interleaving half-circles
- **Blobs** - Gaussian clusters

#### Regression
1. Select **"Regression"** task
2. Select **"Manual"** dataset
3. Choose Delta Rule or MLP
4. Add points to define target function
5. Train and watch the curve fit

**Built-in Presets:**
- Sine Wave
- Parabola
- Linear
- Absolute Value

---

### 2ï¸âƒ£ **MNIST Mode** (Digit Recognition)

Train on 60,000 handwritten digits with professional visualizations.

#### Standard MLP Training
1. Select **"MNIST"** dataset
2. Choose **"Multi-Layer MLP"**
3. Configure architecture (e.g., `784,256,10`)
4. Set stopping criteria (default: Min Error 0.01)
5. Press **"START TRAINING"**
6. Monitor error graphs in real-time

**After Training:**
- **Save Model** - Export trained weights
- **Load Model** - Import previously saved model
- **Test Handwriting** - Draw digits and get predictions

#### Autoencoder Workflow (Two-Stage)

**Stage 1: Train Encoder**
1. Select **"MNIST"** dataset
2. Choose **"Autoencoder-Based MLP"**
3. Configure encoder architecture (e.g., `784,128`)
4. Press **"START TRAINING"**
5. Wait for Stage 1 completion popup
6. **Save Encoder** for reuse
7. Press **"START TRAINING"** again for Stage 2

**Stage 2: Train MLP**
- Uses pre-trained encoder as feature extractor
- Trains smaller MLP on compressed features (e.g., `128,64,10`)
- Faster training with better generalization

**Visualization:**
- **Autoencoder Error** - Reconstruction loss graph
- **MLP Error** - Classification loss graph
- **Reconstruction** - View 10 original vs reconstructed digits

---

## ğŸ—ï¸ Technical Architecture

### Core Algorithms

#### 1. **Perceptron**
```
Update Rule: w = w + Î· * (y_true - y_pred) * x
Activation: Step function
Use Case: Binary/Multi-class classification
```

#### 2. **Delta Rule (Adaline)**
```
Loss (MSE): L = (1/n) * Î£(y_true - y_pred)Â²
Gradient: âˆ‚L/âˆ‚w = -(2/n) * X^T * (y_true - y_pred)
Optimization: Gradient Descent
Use Case: Regression, Linear classification
```

#### 3. **Multi-Layer Perceptron (MLP)**

**Forward Propagation:**
```
Z[l] = A[l-1] @ W[l] + b[l]
A[l] = activation(Z[l])
```

**Backpropagation:**
```
dZ[L] = A[L] - Y  (Output layer)
dW[l] = (1/m) * A[l-1]^T @ dZ[l]
db[l] = (1/m) * Î£ dZ[l]
dZ[l-1] = (dZ[l] @ W[l]^T) * Ïƒ'(Z[l-1])
```

**Activation Functions:**
```python
ReLU:    f(x) = max(0, x)
Tanh:    f(x) = tanh(x)
Sigmoid: f(x) = 1 / (1 + e^-x)
Softmax: f(x_i) = e^x_i / Î£ e^x_j
```

**Loss & Optimization:**
```
Cross-Entropy: L = -(1/m) * Î£ Î£ y_true * log(y_pred)
L2 Regularization: L_reg = (Î»/2m) * Î£||W||Â²
Momentum: v = Î²*v + (1-Î²)*âˆ‡W
```

#### 4. **Autoencoder**
- **Encoder**: Compresses input to latent representation
- **Decoder**: Reconstructs input from latent space
- **Training**: MSE between input and reconstruction
- **Feature Extraction**: Use encoder weights for classification

---

## ğŸ“ Project Structure

```
MLP-From-Scratch-GUI/
â”‚
â”œâ”€â”€ algorithms/          # Neural network implementations
â”‚   â”œâ”€â”€ mlp.py          # Multi-Layer Perceptron
â”‚   â”œâ”€â”€ perceptron.py   # Single-layer Perceptron
â”‚   â”œâ”€â”€ delta_rule.py   # Adaline algorithm
â”‚   â”œâ”€â”€ autoencoder.py  # Autoencoder implementation
â”‚   â””â”€â”€ mlp_with_encoder.py  # Hybrid model
â”‚
â”œâ”€â”€ gui/                # User interface
â”‚   â”œâ”€â”€ control_panel.py       # Hyperparameter controls
â”‚   â”œâ”€â”€ visualization_frames.py # Plots and graphs
â”‚   â”œâ”€â”€ training_manager.py    # Training orchestration
â”‚   â””â”€â”€ handwriting_tester.py  # MNIST drawing canvas
â”‚
â”œâ”€â”€ utils/              # Helper functions
â”‚   â”œâ”€â”€ data_handler.py # Dataset management
â”‚   â”œâ”€â”€ load_mnist.py   # MNIST loader
â”‚   â””â”€â”€ activations.py  # Activation functions
â”‚
â”œâ”€â”€ config/             # Configuration
â”‚   â””â”€â”€ default_hyperparams.py  # Default settings
â”‚
â”œâ”€â”€ dataset/            # Data storage
â”‚   â””â”€â”€ MNIST/         # MNIST binary files
â”‚
â”œâ”€â”€ weights/           # Saved models (auto-created)
â”‚
â”œâ”€â”€ main.py            # Application entry point
â”œâ”€â”€ requirements.txt   # Python dependencies
â””â”€â”€ README.md          # This file
```

---

## ğŸ¯ Use Cases

- **Educational** - Learn neural network fundamentals from scratch
- **Experimentation** - Test architectures and hyperparameters
- **Visualization** - Understand how networks learn decision boundaries
- **Research** - Prototype custom learning algorithms
- **Teaching** - Demonstrate ML concepts interactively

---


## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---


## ğŸ“§ Contact

**Ensar Yesir** - [@ensaryesir](https://github.com/ensaryesir)

Project Link: [https://github.com/ensaryesir/MLP-From-Scratch-GUI](https://github.com/ensaryesir/MLP-From-Scratch-GUI)

---

â­ **Star this repo if you find it useful!**