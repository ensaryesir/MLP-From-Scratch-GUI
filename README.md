# ğŸ§  Neural Network Visualizer - MLP From Scratch

**Profesyonel, interaktif sinir aÄŸÄ± gÃ¶rselleÅŸtirme uygulamasÄ±**

Python ve NumPy kullanÄ±larak sÄ±fÄ±rdan yazÄ±lmÄ±ÅŸ tek katmanlÄ± ve Ã§ok katmanlÄ± sinir aÄŸÄ± algoritmalarÄ±nÄ± gÃ¶rselleÅŸtiren modern masaÃ¼stÃ¼ uygulamasÄ±.

![alt text](image.png)

**âš ï¸ EÄŸitim AmaÃ§lÄ± Proje**: HiÃ§bir ML kÃ¼tÃ¼phanesi kullanÄ±lmadÄ± (scikit-learn, TensorFlow, PyTorch, Keras). TÃ¼m algoritmalar sÄ±fÄ±rdan NumPy ile yazÄ±lmÄ±ÅŸtÄ±r.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![NumPy](https://img.shields.io/badge/NumPy-1.24+-green.svg)
![CustomTkinter](https://img.shields.io/badge/CustomTkinter-5.2+-orange.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

## ğŸ“‹ Ä°Ã§indekiler

- [Ã–zellikler](#-Ã¶zellikler)
- [Kurulum](#-kurulum)
- [KullanÄ±m](#-kullanÄ±m)
- [Veri AkÄ±ÅŸ ÅemasÄ±](#-veri-akÄ±ÅŸ-ÅŸemasÄ±)
- [Proje YapÄ±sÄ±](#-proje-yapÄ±sÄ±)
- [Algoritmalar](#-algoritmalar)
- [KatkÄ±da Bulunma](#-katkÄ±da-bulunma)

## âœ¨ Ã–zellikler

### ğŸ¯ Ä°nteraktif Veri Ekleme
- Fare ile doÄŸrudan grafik Ã¼zerine tÄ±klayarak veri noktalarÄ± ekleyin
- Ã‡oklu sÄ±nÄ±f desteÄŸi (maksimum 6 sÄ±nÄ±f)
- Dinamik sÄ±nÄ±f yÃ¶netimi

### ğŸ¤– ÃœÃ§ FarklÄ± Algoritma
1. **Single-Layer Perceptron**: DoÄŸrusal olarak ayrÄ±labilir problemler iÃ§in
2. **Single-Layer Delta Rule (Adaline)**: MSE minimize eden Widrow-Hoff Ã¶ÄŸrenme kuralÄ±
3. **Multi-Layer Perceptron (MLP)**: Backpropagation ile eÄŸitilen derin sinir aÄŸÄ±

### ğŸ¨ CanlÄ± GÃ¶rselleÅŸtirme
- **Training Sekmesi**: EÄŸitim sÄ±rasÄ±nda karar sÄ±nÄ±rlarÄ±nÄ±n canlÄ± animasyonu
- **Test Sekmesi**: Test verisi Ã¼zerinde model performansÄ±
- **Error Graph**: Epoch'lara gÃ¶re error deÄŸiÅŸimi

### âš™ï¸ Esnek Hiperparametre KontrolÃ¼
- Ã–zelleÅŸtirilebilir katman mimarisi
- Aktivasyon fonksiyonu seÃ§imi (ReLU, Tanh, Sigmoid, Softmax)
- Ã–ÄŸrenme oranÄ±, epoch sayÄ±sÄ±, batch size ayarlarÄ±
- L2 Regularization desteÄŸi
- Test/Train split oranÄ±

### ğŸ”¬ SÄ±fÄ±rdan YazÄ±lmÄ±ÅŸ Algoritmalar
- **Backpropagation**: Chain rule ile manuel gradyan hesaplama
- **Aktivasyon FonksiyonlarÄ±**: ReLU, Tanh, Sigmoid ve tÃ¼revleri
- **Loss Fonksiyonu**: Cross-Entropy Loss
- **Optimization**: Mini-batch Gradient Descent
- **Regularization**: L2 (Weight Decay)

## ğŸš€ Kurulum

### Gereksinimler
- Python 3.8 veya Ã¼zeri
- pip paket yÃ¶neticisi

### AdÄ±m 1: Depoyu KlonlayÄ±n
```bash
git clone https://github.com/ensaryesir/MLP-From-Scratch-GUI.git
cd MLP-From-Scratch-GUI
```

### AdÄ±m 2: BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kleyin
```bash
pip install -r requirements.txt
```

### AdÄ±m 3: UygulamayÄ± Ã‡alÄ±ÅŸtÄ±rÄ±n
```bash
python main.py
```

## ğŸ“– KullanÄ±m

### ğŸ¯ Temel AdÄ±mlar
1. **Veri Ekleme**: Training sekmesinde grafiÄŸe tÄ±klayarak veri noktalarÄ± ekleyin
2. **Model SeÃ§imi**: Perceptron, Delta Rule veya MLP seÃ§in
3. **Hiperparametre AyarlarÄ±**: Learning rate, epochs, architecture ayarlayÄ±n
4. **EÄŸitim**: START TRAINING butonuna tÄ±klayÄ±n
5. **Ä°zleme**: Error Graph'te loss deÄŸiÅŸimini, Training'de karar sÄ±nÄ±rlarÄ±nÄ± takip edin
6. **DeÄŸerlendirme**: Test sekmesinde model performansÄ±nÄ± gÃ¶rÃ¼n

### ğŸ’¡ Ä°puÃ§larÄ±
- **Veri**: SÄ±nÄ±flar arasÄ± dengeli nokta sayÄ±sÄ±, farklÄ± bÃ¶lgelere daÄŸÄ±tÄ±m
- **Model**: Basit problemler â†’ Perceptron/Delta Rule, Non-linear â†’ MLP
- **EÄŸitim**: Error azalmÄ±yor â†’ Learning rate artÄ±r, Error sallanÄ±yor â†’ Learning rate dÃ¼ÅŸÃ¼r

### ğŸ”§ Ã–rnek Senaryolar

#### Senaryo 1: XOR Problemi
```
1. Ä°ki sÄ±nÄ±f oluÅŸturun
2. Veri: (2,2)â†’Class0, (8,8)â†’Class0, (2,8)â†’Class1, (8,2)â†’Class1
3. Model: Multi-Layer (MLP)
4. Mimari: 2,8,2
5. Aktivasyon: relu,softmax
6. Ã–ÄŸrenme OranÄ±: 0.1
7. Epochs: 300
```

#### Senaryo 2: 3 SÄ±nÄ±flÄ± Classification
```
1. ÃœÃ§ sÄ±nÄ±f oluÅŸturun
2. Her sÄ±nÄ±ftan 15-20 nokta ekleyin
3. Model: Multi-Layer (MLP)
4. Mimari: 2,10,3
5. Aktivasyon: tanh,softmax
6. Ã–ÄŸrenme OranÄ±: 0.05
7. Epochs: 200
```

## ğŸ”„ Veri AkÄ±ÅŸ ÅemasÄ±

### ğŸ“Š DetaylÄ± Veri AkÄ±ÅŸ Tablosu
```
ğŸ‘¤KullanÄ±cÄ± â†’ ğŸ›ï¸ Kontrol Paneli â†’ ğŸ§  main.py â†’ ğŸ¤– Algoritmalar â†’ ğŸ§  main.py â†’ ğŸ“Š GÃ¶rselleÅŸtirme
      â†“                 â†“                â†“               â†“              â†“                 â†“
  Mouse Click      Hyperparams       Orchestrate     Training        Receive         Real-time
  Add Points       Learning Rate     Create Model    Forward Pass    Results         Error Graph
  Select Class     Epochs            Start Train     Backward Pass   Coordinate      Decision Boundary
  Class Mgmt       Architecture      Async Loop      Update Weights  Update UI       Test Results
  Buttons          Activations       Coordinate      Yield Results   Distribute      Live Animation
  Clear Data       Batch Size        Get Settings    Compute Loss    Send Data       Tab Switching
  Training         L2 Lambda         Build Model     Fit Data        Control         Status Updates
  Settings         Test Split        Run Epochs      Generate        Manage          Plot Updates
``` 

### ğŸ”— BaÄŸlantÄ± MekanizmalarÄ±

**1. Callback Pattern (Control Panel â†’ Main):**
- Button click â†’ callback trigger â†’ main.py method Ã§aÄŸrÄ±sÄ±

**2. Parameter Passing (Main â†’ Algorithm):**
- Hyperparameter'larÄ± topla â†’ model constructor'a geÃ§ir

**3. Generator Pattern (Algorithm â†’ Main):**
- Her epoch'ta yield â†’ non-blocking execution â†’ UI responsive

**4. Direct Calls (Main â†’ Visualization):**
- SonuÃ§larÄ± al â†’ gÃ¶rselleÅŸtirme method'larÄ±nÄ± Ã§aÄŸÄ±r

**Bu mimari sayesinde:**
- âœ… **ModÃ¼ler**: Her component baÄŸÄ±msÄ±z
- âœ… **Responsive**: Non-blocking UI  
- âœ… **Extensible**: Yeni algoritmalar kolayca eklenebilir
- âœ… **Maintainable**: Clear separation of concerns

## ğŸ“ Proje YapÄ±sÄ±

```
MLP-From-Scratch-GUI/
â”œâ”€â”€ main.py                      # Ana uygulama orkestratÃ¶rÃ¼
â”œâ”€â”€ requirements.txt             # Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â”œâ”€â”€ README.md                    # Proje dokÃ¼mantasyonu
â”‚
â”œâ”€â”€ algorithms/                  # Sinir aÄŸÄ± algoritmalarÄ±
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ single_layer.py         # Perceptron ve Delta Rule
â”‚   â””â”€â”€ mlp.py                  # Multi-Layer Perceptron + Backpropagation
â”‚
â”œâ”€â”€ gui/                         # KullanÄ±cÄ± arayÃ¼zÃ¼ bileÅŸenleri
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ control_panel.py        # Kontrol paneli widget'larÄ±
â”‚   â””â”€â”€ visualization_frames.py # GÃ¶rselleÅŸtirme sekmeleri
â”‚
â””â”€â”€ utils/                       # YardÄ±mcÄ± modÃ¼ller
    â”œâ”€â”€ __init__.py
    â””â”€â”€ data_handler.py         # Veri yÃ¶netimi
```

### ğŸ“ˆ Test SenaryolarÄ±

**Senaryo 1: Linear Problem (BaÅŸarÄ±lÄ± âœ“)**
- 2 sÄ±nÄ±f, doÄŸrusal ayrÄ±labilir
- Perceptron ile hÄ±zlÄ± yakÄ±nsama
- Accuracy: ~100%

**Senaryo 2: XOR Problemi (BaÅŸarÄ±lÄ± âœ“)**
- 2 sÄ±nÄ±f, non-linear
- MLP (2,8,2) ile Ã§Ã¶zÃ¼m
- Accuracy: ~95-100%

**Senaryo 3: Multi-Class (BaÅŸarÄ±lÄ± âœ“)**
- 3-6 sÄ±nÄ±f
- MLP ile kompleks karar sÄ±nÄ±rlarÄ±
- Accuracy: Model ve veriye baÄŸlÄ±

## ğŸ§® Algoritmalar

### Perceptron
```
GÃ¼ncelleme KuralÄ±: w = w + Î· * (y_true - y_pred) * x
```
- Step aktivasyon fonksiyonu
- Binary ve multi-class classification desteÄŸi

### Delta Rule (Adaline)
```
Loss: MSE = (1/n) * Î£(y_true - y_pred)Â²
Gradient: âˆ‚L/âˆ‚w = -(2/n) * X^T * (y_true - y_pred)
```
- Linear aktivasyon fonksiyonu
- Gradient descent ile eÄŸitim

### Multi-Layer Perceptron (MLP)

**Forward Propagation:**
```
Z^[l] = A^[l-1] * W^[l] + b^[l]
A^[l] = activation(Z^[l])
```

**Backpropagation:**
```
dZ^[L] = A^[L] - Y  (son katman)
dW^[l] = (1/m) * A^[l-1]^T * dZ^[l]
db^[l] = (1/m) * Î£(dZ^[l])
dZ^[l-1] = dZ^[l] * W^[l]^T âŠ™ g'(Z^[l-1])
```

**Aktivasyon FonksiyonlarÄ±:**
- **ReLU**: `f(x) = max(0, x)`, `f'(x) = 1 if x > 0 else 0`
- **Tanh**: `f(x) = tanh(x)`, `f'(x) = 1 - tanhÂ²(x)`
- **Sigmoid**: `f(x) = 1/(1+e^-x)`, `f'(x) = f(x)(1-f(x))`
- **Softmax**: `f(x_i) = e^x_i / Î£e^x_j` (multi-class iÃ§in)

**Loss Fonksiyonu:**
```
Cross-Entropy: L = -(1/m) * Î£ Î£ y_true * log(y_pred)
L2 Regularization: L_reg = (Î»/2m) * Î£||W||Â²
```

## ğŸ¤ KatkÄ±da Bulunma

KatkÄ±larÄ±nÄ±zÄ± bekliyorum! LÃ¼tfen ÅŸu adÄ±mlarÄ± izleyin:

1. Projeyi fork edin
2. Feature branch oluÅŸturun (`git checkout -b feature/AmazingFeature`)
3. DeÄŸiÅŸikliklerinizi commit edin (`git commit -m 'Add some AmazingFeature'`)
4. Branch'inizi push edin (`git push origin feature/AmazingFeature`)
5. Pull Request aÃ§Ä±n

## ğŸ“ Lisans

Bu proje MIT lisansÄ± altÄ±nda lisanslanmÄ±ÅŸtÄ±r. Detaylar iÃ§in `LICENSE` dosyasÄ±na bakÄ±n.

## ğŸ‘¨â€ğŸ’» GeliÅŸtirici

**Ensar Yesir**
- GitHub: [@ensaryesir](https://github.com/ensaryesir)

â­ Projeyi beÄŸendiyseniz yÄ±ldÄ±z vermeyi unutmayÄ±n!

**Keyifli kodlamalar! ğŸš€**